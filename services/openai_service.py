import os
from typing import Optional
from fastapi import HTTPException
from dotenv import load_dotenv
import openai
from datetime import datetime

# Import necessary packages
from openai import OpenAIError
from openai.types import ChatCompletionRequestMessage

# Load environment variables from .env file
load_dotenv()

# API Key for OpenAI (from .env file)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# OpenAI service for generating responses
openai_service = openai.OpenAI(api_key=OPENAI_API_KEY)

# Constants for caching (defined in `helpers.py`)
CACHE_TTL_SECONDS = 3600  # Cache validity in seconds
CACHE_MAX_SIZE = 1000  # Maximum number of cached responses

# Initialize the cache (using a simple dictionary for MVP)
CACHE = {}

class OpenAI:
    """
    The OpenAI service class, responsible for interacting with the OpenAI API.

    This class provides a `generate_response` method to process user requests and obtain responses from OpenAI.
    """

    def __init__(self, api_key: str):
        """
        Initializes the OpenAI service with the provided API key.

        Args:
            api_key (str): The OpenAI API key.
        """
        self.api_key = api_key

    async def generate_response(self, text: str) -> str:
        """
        Processes a user text request and generates a response using OpenAI's API.

        Args:
            text (str): The user's text request.

        Returns:
            str: The response generated by OpenAI.

        Raises:
            HTTPException: If an error occurs during API communication.
        """
        # 1. Check for cached responses:
        cached_response = self._get_cached_response(text)
        if cached_response:
            return cached_response

        # 2. Send the request to OpenAI API:
        try:
            response = await openai_service.chat.completions.create(
                model="gpt-3.5-turbo",  # Choose the OpenAI model
                messages=[
                    {"role": "user", "content": text}  # Format the user request
                ],
                temperature=0.7,  # Adjust the creativity of the response
                max_tokens=1000,  # Limit the length of the response
            )
        except OpenAIError as e:
            raise HTTPException(status_code=500, detail=f"OpenAI API request failed: {e}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Internal server error: {e}")

        # 3. Format and return the response:
        formatted_response = self._format_response(response)

        # 4. Cache the response for future use:
        self._cache_response(text, formatted_response)

        return formatted_response

    def _get_cached_response(self, text: str) -> Optional[str]:
        """
        Retrieves a cached response based on the user request text.

        Args:
            text (str): The user's text request.

        Returns:
            Optional[str]: The cached response if found, otherwise None.
        """
        global CACHE
        cache_key = text
        if cache_key in CACHE:
            response, cached_timestamp = CACHE[cache_key]
            if (datetime.utcnow() - cached_timestamp).total_seconds() <= CACHE_TTL_SECONDS:
                return response
        return None

    def _cache_response(self, text: str, response: str) -> None:
        """
        Caches the generated response based on the user request text.

        Args:
            text (str): The user's text request.
            response (str): The generated response from OpenAI.
        """
        global CACHE
        cache_key = text
        CACHE[cache_key] = (response, datetime.utcnow())
        if len(CACHE) > CACHE_MAX_SIZE:
            self._cleanup_cache()

    def _cleanup_cache(self) -> None:
        """
        Removes expired or unused cached data to manage cache size and efficiency.
        """
        global CACHE
        for cache_key, (response, cached_timestamp) in list(CACHE.items()):
            if (datetime.utcnow() - cached_timestamp).total_seconds() > CACHE_TTL_SECONDS:
                del CACHE[cache_key]

    def _format_response(self, response: dict) -> str:
        """
        Formats the response from the OpenAI API into a user-friendly format.

        Args:
            response (dict): The raw response from the OpenAI API.

        Returns:
            str: The formatted response.
        """
        if isinstance(response, str):
            return response
        elif isinstance(response, dict) and "choices" in response:
            return response["choices"][0]["message"]["content"]
        else:
            return json.dumps(response, indent=2)

# Create a global instance of the OpenAI service
openai_service = OpenAI(api_key=OPENAI_API_KEY)